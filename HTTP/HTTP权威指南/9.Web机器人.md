# Web 机器人

Web 爬虫是一种机器人，它们会递归地对各种信息性 Web 站点进行遍历，获取第一个 Web 页面，然后获取那个页面指向的所有 Web 页面，然后是那些页面指向的所有 Web 页面，依此类推。递归地追踪这些 Web 链接的机器人会沿着 HTML 超链创建的网络“爬行”，所以将其称为爬虫（crawler）或蜘蛛（spider）。

1. 爬虫开始访问的 URL 初始集合被称作根集（root set）。
2. 爬虫在 Web 上移动时，会不停地对 HTML 页面进行解析。它要对所解析的每个页面上的 URL 链接进行分析，并将这些链接添加到需要爬行的页面列表中去。
3. 机器人在 Web 上爬行时，要特别小心不要陷入循环，或环路（cycle）之中。机器人必须知道它们到过何处。

## 管理访问过的地址的技术

- 树和散列表：记录已访问的URL
- 有损的存在位图：用一个散列函数将每个 URL 都转换成一个定长的数字，这个数字在数组中有个相关的“存在位
- 检查点：将已访问 URL 保存在硬盘上，防止机器人程序崩溃
- 分类：多个机器人分工合作

如果两个 URL 看起来不一样，但实际指向的是同一资源，就称这两个 URL 互为“别名”。

## 避免循环和重复

- 规范化 URL
- 广度优先的爬行
- 节流
- 限制 URL 的大小
- URL / 站点黑名单
- 模式检测
- 内容指纹
- 人工监视

## 拒绝机器人访问

通过 robots.txt 的规范实现。

## 搜索引擎

